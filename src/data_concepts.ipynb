{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "# Reuse paths and constants from training.py\n",
    "SRC_DIR = Path.cwd()\n",
    "ROOT_DIR = SRC_DIR.parent\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'dataset')\n",
    "PREPROCESSED_DIR = os.path.join(DATA_DIR, 'preprocessed')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'csv_mappings', 'train.csv')\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "CBM_DIR = os.path.join(MODEL_DIR, 'cbm')\n",
    "os.makedirs(CBM_DIR, exist_ok=True)\n",
    "\n",
    "# Extended Dataset for Concept Labels\n",
    "class ConceptMushroomDataset(Dataset):\n",
    "    def __init__(self, preprocessed_dir, csv_path, transform=None):\n",
    "        self.preprocessed_dir = preprocessed_dir\n",
    "        self.csv_data = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "\n",
    "        # Images and Labels\n",
    "        self.image_ids = self.csv_data['Image'].values\n",
    "        self.labels = self.csv_data['Mushroom'].values\n",
    "        self.concepts = self.csv_data.iloc[:, 2:].values  # Assuming concepts are columns 2+\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = str(self.image_ids[idx]).zfill(5)\n",
    "        label = self.labels[idx]\n",
    "        concepts = torch.tensor(self.concepts[idx], dtype=torch.float32)\n",
    "\n",
    "        # Load image\n",
    "        image_path = os.path.join(self.preprocessed_dir, f\"{image_id}.pt\")\n",
    "        image = torch.load(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, concepts\n",
    "\n",
    "# CBM Model Definition\n",
    "class ConceptBottleneckModel(nn.Module):\n",
    "    def __init__(self, num_concepts, num_classes):\n",
    "        super(ConceptBottleneckModel, self).__init__()\n",
    "\n",
    "        # Feature extractor (ResNet)\n",
    "        self.feature_extractor = models.resnet50(pretrained=True)\n",
    "        self.feature_extractor.fc = nn.Linear(self.feature_extractor.fc.in_features, num_concepts)\n",
    "\n",
    "        # Bottleneck to Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_concepts, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        concepts = self.feature_extractor(x)\n",
    "        predictions = self.classifier(concepts)\n",
    "        return concepts, predictions\n",
    "\n",
    "# Data Preparation\n",
    "def get_data_loaders(preprocessed_dir, csv_path, batch_size=32):\n",
    "    # Init\n",
    "    dataset = ConceptMushroomDataset(preprocessed_dir, csv_path)\n",
    "\n",
    "    # Split \n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, temp_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "    val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "    # Subsets\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    val_subset = Subset(dataset, val_indices)\n",
    "    test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "    # Dataloaders\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# Training Loop for Concepts\n",
    "def train_concept_layer(model, train_loader, val_loader, device, epochs=10):\n",
    "    criterion = nn.MSELoss()  # Mean squared error for concept predictions\n",
    "    optimizer = optim.Adam(model.feature_extractor.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for images, _, concepts in tqdm(train_loader, desc=f\"[Train Concepts: Epoch {epoch+1}/{epochs}]\"):\n",
    "            images, concepts = images.to(device), concepts.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predicted_concepts, _ = model(images)\n",
    "            loss = criterion(predicted_concepts, concepts)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Training Loop for Classification\n",
    "def train_classification_layer(model, train_loader, val_loader, device, epochs=10):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for images, labels, _ in tqdm(train_loader, desc=f\"[Train Classification: Epoch {epoch+1}/{epochs}]\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            _, predictions = model(images)\n",
    "            loss = criterion(predictions, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Loss = {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_cbm(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in tqdm(test_loader, desc=\"[Evaluate]\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            _, predictions = model(images)\n",
    "            \n",
    "            _, predicted_labels = torch.max(predictions, 1)\n",
    "            total_correct += (predicted_labels == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * total_correct / total_samples\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Concept Layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train Concepts: Epoch 1/10]:   0%|          | 0/52 [00:00<?, ?it/s]C:\\Users\\ilian\\AppData\\Local\\Temp\\ipykernel_6988\\2525085129.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  image = torch.load(image_path)\n",
      "c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([32, 0])) that is different to the input size (torch.Size([32, 20])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "[Train Concepts: Epoch 1/10]:   0%|          | 0/52 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (20) must match the size of tensor b (0) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m ConceptBottleneckModel(num_concepts, num_classes)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Concept Layer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mtrain_concept_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Classification Layer\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m train_classification_layer(model, train_loader, val_loader, device)\n",
      "Cell \u001b[1;32mIn[4], line 108\u001b[0m, in \u001b[0;36mtrain_concept_layer\u001b[1;34m(model, train_loader, val_loader, device, epochs)\u001b[0m\n\u001b[0;32m    106\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m    107\u001b[0m predicted_concepts, _ \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m--> 108\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredicted_concepts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcepts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    110\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[1;32mc:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:608\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreduction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:3791\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3788\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3789\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3791\u001b[0m expanded_input, expanded_target \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mmse_loss(\n\u001b[0;32m   3793\u001b[0m     expanded_input, expanded_target, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction)\n\u001b[0;32m   3794\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\functional.py:76\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[38;5;241m*\u001b[39mtensors)\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbroadcast_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (20) must match the size of tensor b (0) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_loader, val_loader, test_loader = get_data_loaders(PREPROCESSED_DIR, CSV_PATH)\n",
    "\n",
    "num_concepts = 20  # Example concept count\n",
    "num_classes = len(pd.read_csv(CSV_PATH)['Mushroom'].unique())\n",
    "\n",
    "model = ConceptBottleneckModel(num_concepts, num_classes).to(device)\n",
    "\n",
    "print(\"Training Concept Layer\")\n",
    "train_concept_layer(model, train_loader, val_loader, device)\n",
    "\n",
    "print(\"Training Classification Layer\")\n",
    "train_classification_layer(model, train_loader, val_loader, device)\n",
    "\n",
    "print(\"Evaluating Model\")\n",
    "evaluate_cbm(model, test_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
