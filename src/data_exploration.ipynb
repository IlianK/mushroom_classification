{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Mushroom Dataset (with mapping to csv with labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseMushroomDataset(Dataset):\n",
    "    def __init__(self, csv_file, root_dir, transform=None, has_labels=True):\n",
    "        self.annotations = pd.read_csv(csv_file, dtype={0: str})\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.has_labels = has_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.root_dir, self.annotations.iloc[idx, 0] + '.jpg')\n",
    "        image = Image.open(img_name).convert(\"RGB\")\n",
    "        if self.has_labels:\n",
    "            label = int(self.annotations.iloc[idx, 1])\n",
    "        else:\n",
    "            label = -1  \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Adding white padding around images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_to_square(image):\n",
    "    width, height = image.size\n",
    "    max_dim = max(width, height)\n",
    "    left_padding = (max_dim - width) // 2\n",
    "    top_padding = (max_dim - height) // 2\n",
    "    padding = (left_padding, top_padding, max_dim - width - left_padding, max_dim - height - top_padding)\n",
    "    return F.pad(image, padding, 255, 'constant')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Transform images to uniform size 224x224 (for AlexNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Lambda(pad_to_square),\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base paths\n",
    "root_path = os.path.dirname(os.getcwd())\n",
    "dataset_path = os.path.join(root_path, 'dataset')\n",
    "dataset_raw_path = os.path.join(dataset_path, 'raw')\n",
    "dataset_preprocessed_path = os.path.join(dataset_path, 'preprocessed')\n",
    "csv_path = os.path.join(dataset_path, 'csv_mappings')\n",
    "\n",
    "# Target preprocessing folder\n",
    "preprocessed_train_path = os.path.join(root_path, 'dataset', 'preprocessed', 'train')\n",
    "os.makedirs(preprocessed_train_path, exist_ok=True)\n",
    "\n",
    "preprocessed_test_path = os.path.join(root_path, 'dataset', 'preprocessed', 'test')\n",
    "os.makedirs(preprocessed_test_path, exist_ok=True)\n",
    "\n",
    "# Mappings to names in CSV\n",
    "os.makedirs(csv_path, exist_ok=True)\n",
    "train_csv_path = os.path.join(csv_path, 'train.csv')\n",
    "test_csv_path = os.path.join(csv_path, 'test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = BaseMushroomDataset(csv_file=train_csv_path, root_dir=dataset_raw_path, transform=transform, has_labels=True)\n",
    "test_dataset = BaseMushroomDataset(csv_file=test_csv_path, root_dir=dataset_raw_path, transform=transform, has_labels=False)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "single_image, single_label = train_dataset[0]  \n",
    "print(f\"Data Shape: {single_image.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(image, mean, std):\n",
    "    image = image.clone()\n",
    "    for t, m, s in zip(image, mean, std):\n",
    "        t.mul_(s).add_(m)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(dataset, num_samples=10, images_per_row=5):\n",
    "    num_rows = (num_samples + images_per_row - 1) // images_per_row  \n",
    "    fig, axes = plt.subplots(num_rows, images_per_row, figsize=(20, num_rows * 4))\n",
    "    axes = axes.flatten() \n",
    "\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image, label = dataset[i]\n",
    "        image = denormalize(image, mean, std)\n",
    "        image = image.permute(1, 2, 0).numpy()  \n",
    "        image = np.clip(image, 0, 1)           \n",
    "        \n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f'Label: {label}')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    for i in range(num_samples, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(train_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_samples(test_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Save preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_preprocessed_images_as_tensors(dataset, save_dir):\n",
    "    for idx in range(len(dataset)):\n",
    "        image, label = dataset[idx]\n",
    "        img_name = dataset.annotations.iloc[idx, 0] + '.pt'\n",
    "        torch.save(image, os.path.join(save_dir, img_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_preprocessed_images_as_tensors(train_dataset, preprocessed_train_path)\n",
    "save_preprocessed_images_as_tensors(test_dataset, preprocessed_test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test loading preprocessed again (from tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_preprocessed_samples(preprocessed_path, num_samples=25, images_per_row=5):\n",
    "    image_files = [f for f in os.listdir(preprocessed_path) if f.endswith('.pt')]\n",
    "    num_samples = min(num_samples, len(image_files))\n",
    "    \n",
    "    num_rows = (num_samples + images_per_row - 1) // images_per_row  \n",
    "    fig, axes = plt.subplots(num_rows, images_per_row, figsize=(20, num_rows * 4))\n",
    "    axes = axes.flatten() \n",
    "\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        image_tensor = torch.load(os.path.join(preprocessed_path, image_files[i]))\n",
    "        image_tensor = denormalize(image_tensor, mean, std)\n",
    "        image = transforms.ToPILImage()(image_tensor)  \n",
    "        axes[i].imshow(image)\n",
    "        axes[i].set_title(f'Image: {image_files[i]}')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    for i in range(num_samples, len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_preprocessed_samples(preprocessed_train_path)\n",
    "show_preprocessed_samples(preprocessed_test_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
