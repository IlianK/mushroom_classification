{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torchcam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "from torchcam.methods import GradCAM\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x23cb69ce150>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR = Path.cwd()\n",
    "ROOT_DIR = SRC_DIR.parent\n",
    "\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'dataset')\n",
    "PREPROCESSED_DIR = os.path.join(DATA_DIR, 'preprocessed')\n",
    "CSV_PATH = os.path.join(DATA_DIR, 'csv_mappings', 'train.csv')\n",
    "\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "BASELINE_DIR = os.path.join(MODEL_DIR, 'baselines')\n",
    "BASELINE_FINETUNED_DIR = os.path.join(MODEL_DIR, 'baselines_finetuned')\n",
    "RESULT_DIR = os.path.join(BASELINE_DIR, 'results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = {\n",
    "    0: \"amanita\",\n",
    "    1: \"boletus\",\n",
    "    2: \"chantelle\",\n",
    "    3: \"deterrimus\",\n",
    "    4: \"rufus\",\n",
    "    5: \"torminosus\",\n",
    "    6: \"aurantiacum\",\n",
    "    7: \"procera\",\n",
    "    8: \"involutus\",\n",
    "    9: \"russula\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = training.get_data_loaders(PREPROCESSED_DIR, CSV_PATH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Set model to explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "NUM_CLASSES = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'resnet'  \n",
    "# alexnet # resnet # vgg16 # densenet # efficientnet\n",
    "# custom_alexnet custom_resnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\src\\training.py:384: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "model = training.load_model_for_explaining(model_type, NUM_CLASSES, DEVICE, finetuned=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\src\\training.py:74: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  image = torch.load(image_path)\n"
     ]
    }
   ],
   "source": [
    "random_index = random.randint(0, len(test_loader.dataset) - 1)\n",
    "image, label = test_loader.dataset[random_index]\n",
    "image_tensor = image.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize Grad-CAM\n",
    "gradcam = GradCAM(model, target_layers=[model.layer4])  # Replace with your model's layer if necessary\n",
    "\n",
    "# Step 2: Preprocess the image for model input (Ensure it's already a tensor with required transformations)\n",
    "# Since the image is already a tensor, we skip transformations. If it's in PIL format, you'd apply the transform.\n",
    "image_tensor = image_tensor.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Step 3: Perform Grad-CAM and get the heatmap\n",
    "grayscale_cam = gradcam(image_tensor)\n",
    "grayscale_cam = grayscale_cam[0, :]  # Get the first image in batch\n",
    "\n",
    "# Step 4: Convert the grayscale heatmap to a color heatmap\n",
    "heatmap = cv2.applyColorMap(np.uint8(255 * grayscale_cam), cv2.COLORMAP_JET)\n",
    "heatmap = np.float32(heatmap) / 255\n",
    "\n",
    "# Step 5: Get the original image\n",
    "original_image = image.permute(1, 2, 0).cpu().numpy()  # Convert to HWC format\n",
    "\n",
    "# Step 6: Overlay the heatmap on the original image\n",
    "superimposed_img = heatmap + np.float32(original_image)\n",
    "superimposed_img = superimposed_img / np.max(superimposed_img)  # Normalize to [0, 1]\n",
    "\n",
    "# Step 7: Display the original image with the Grad-CAM heatmap overlay\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(superimposed_img)\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
