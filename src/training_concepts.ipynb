{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR = Path.cwd()\n",
    "ROOT_DIR = SRC_DIR.parent\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'dataset')\n",
    "PREPROCESSED_DIR = os.path.join(DATA_DIR, 'preprocessed')\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "CBM_DIR = os.path.join(MODEL_DIR, 'cbm')\n",
    "os.makedirs(CBM_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = os.path.join(DATA_DIR, 'csv_mappings', 'train.csv')\n",
    "CSV_CONCEPTS_PATH = os.path.join(DATA_DIR, 'csv_mappings', 'train_concepts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = {\n",
    "    0: \"amanita\",\n",
    "    1: \"boletus\",\n",
    "    2: \"chantelle\",\n",
    "    3: \"deterrimus\",\n",
    "    4: \"rufus\",\n",
    "    5: \"torminosus\",\n",
    "    6: \"aurantiacum\",\n",
    "    7: \"procera\",\n",
    "    8: \"involutus\",\n",
    "    9: \"russula\"\n",
    "}\n",
    "\n",
    "CONCEPT_LABELS = [\"cap_color\", \"cap_shape\", \"cap_texture\", \"ring_present\"]\n",
    "\n",
    "CONCEPT_VALUES = {\n",
    "    \"cap_color\": [\"red\", \"brown\", \"yellow\", \"white\"],\n",
    "    \"cap_shape\": [\"convex\", \"flat\", \"bulbous\", \"inverted\"],\n",
    "    \"cap_texture\": [\"scaly\", \"smooth\", \"warty\"],\n",
    "    \"ring_present\": [\"yes\", \"no\"]\n",
    "}\n",
    "\n",
    "CONCEPT_MAPPING = {\n",
    "    0: {\"cap_color\": \"red\", \"cap_shape\": \"flat\", \"cap_texture\": \"warty\", \"ring_present\": \"yes\"},            # Amanita \n",
    "    1: {\"cap_color\": \"brown\", \"cap_shape\": \"convex\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},        # Boletus \n",
    "    2: {\"cap_color\": \"yellow\", \"cap_shape\": \"inverted\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},     # Chantelle\n",
    "    3: {\"cap_color\": \"red\", \"cap_shape\": \"flat\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},            # Deterrimus \n",
    "    4: {\"cap_color\": \"brown\", \"cap_shape\": \"inverted\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},      # Rufus \n",
    "    5: {\"cap_color\": \"white\", \"cap_shape\": \"inverted\", \"cap_texture\": \"scaly\", \"ring_present\": \"no\"},       # Torminosus \n",
    "    6: {\"cap_color\": \"brown\", \"cap_shape\": \"convex\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},        # Aurantiacum\n",
    "    7: {\"cap_color\": \"white\", \"cap_shape\": \"bulbous\", \"cap_texture\": \"scaly\", \"ring_present\": \"yes\"},       # Procera\n",
    "    8: {\"cap_color\": \"brown\", \"cap_shape\": \"convex\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},        # Involutus \n",
    "    9: {\"cap_color\": \"red\", \"cap_shape\": \"flat\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},            # Russula \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_labels(mushroom_class):\n",
    "    concepts = CONCEPT_MAPPING.get(mushroom_class, {})\n",
    "    return concepts\n",
    "\n",
    "concepts_list = []\n",
    "for index, row in df.iterrows():\n",
    "    mushroom_class = row['Mushroom']\n",
    "    concepts = get_concept_labels(mushroom_class)\n",
    "    concepts_list.append(concepts)\n",
    "\n",
    "concept_df = pd.DataFrame(concepts_list)\n",
    "df_with_concepts = pd.concat([df, concept_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV with concepts saved to: c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\dataset\\csv_mappings\\train_concepts.csv\n"
     ]
    }
   ],
   "source": [
    "df_with_concepts.to_csv(CSV_CONCEPTS_PATH, index=False)\n",
    "print(f\"New CSV with concepts saved to: {CSV_CONCEPTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptBottleneckModel(nn.Module):\n",
    "    def __init__(self, num_concepts, num_classes):\n",
    "        super(ConceptBottleneckModel, self).__init__()\n",
    "\n",
    "        self.feature_extractor = models.resnet50(pretrained=True)\n",
    "\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.feature_extractor.fc = nn.Linear(self.feature_extractor.fc.in_features, num_concepts)\n",
    "\n",
    "        for param in self.feature_extractor.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_concepts, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        concepts = self.feature_extractor(x)\n",
    "        predictions = self.classifier(concepts)\n",
    "        return concepts, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptMushroomDataset(Dataset):\n",
    "    def __init__(self, preprocessed_dir, csv_path, transform=None):\n",
    "        self.preprocessed_dir = preprocessed_dir\n",
    "        self.csv_data = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_ids = self.csv_data['Image'].values\n",
    "        self.labels = self.csv_data['Mushroom'].values\n",
    "\n",
    "        concept_columns = [\"cap_color\", \"cap_shape\", \"cap_texture\", \"ring_present\"]\n",
    "        self.concepts = pd.get_dummies(self.csv_data[concept_columns]).values.astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = str(self.image_ids[idx]).zfill(5)\n",
    "        label = self.labels[idx]\n",
    "        concepts = torch.tensor(self.concepts[idx], dtype=torch.float32)\n",
    "\n",
    "        image_path = os.path.join(self.preprocessed_dir, f\"{image_id}.pt\")\n",
    "        image = torch.load(image_path, weights_only=True)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(preprocessed_dir, csv_path, batch_size=32):\n",
    "    dataset = ConceptMushroomDataset(preprocessed_dir, csv_path)\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, temp_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "    val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    val_subset = Subset(dataset, val_indices)\n",
    "    test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_concept_accuracy(predicted_concepts, ground_truth):\n",
    "    predicted_classes = torch.argmax(predicted_concepts, dim=1)\n",
    "    true_classes = torch.argmax(ground_truth, dim=1)\n",
    "    correct = (predicted_classes == true_classes).sum().item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_separately_concept(model, train_loader, val_loader, device, optimizer_concept, epochs=10, patience=3):\n",
    "    criterion_concept = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0.0\n",
    "        train_concept_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        print(\"\\n-----------------------------------\")\n",
    "        progress_bar = tqdm(\n",
    "            desc=f\"[Concept Training: Epoch {epoch+1}/{epochs}]\",\n",
    "            total=len(train_loader),\n",
    "            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed}, it/s: {rate_fmt}]\",\n",
    "            ncols=100,\n",
    "            dynamic_ncols=True,\n",
    "            position=0,\n",
    "            leave=True,\n",
    "            mininterval=1\n",
    "        )\n",
    "\n",
    "        for images, labels, concepts in train_loader:\n",
    "            images, concepts = images.to(device), concepts.to(device)\n",
    "\n",
    "            optimizer_concept.zero_grad()\n",
    "            predicted_concepts, _ = model(images)\n",
    "\n",
    "            # Compute concept loss\n",
    "            loss_concept = criterion_concept(predicted_concepts, torch.argmax(concepts, dim=1))\n",
    "            loss_concept.backward()\n",
    "            optimizer_concept.step()\n",
    "\n",
    "            train_total_loss += loss_concept.item()\n",
    "            train_concept_correct += (predicted_concepts.argmax(dim=1) == torch.argmax(concepts, dim=1)).sum().item()\n",
    "            train_total += concepts.size(0)\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'Train Loss': train_total_loss / (len(train_loader) * train_loader.batch_size),\n",
    "                                       'Train Accuracy': 100 * train_concept_correct / train_total})\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        train_concept_accuracy = 100 * train_concept_correct / train_total\n",
    "\n",
    "        print(f\"Train Concept Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"Training Concept Loss: {train_total_loss / len(train_loader):.4f}, Training Concept Accuracy: {train_concept_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_total_loss = 0.0\n",
    "        val_concept_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, concepts in val_loader:\n",
    "                images, concepts = images.to(device), concepts.to(device)\n",
    "\n",
    "                predicted_concepts, _ = model(images)\n",
    "\n",
    "                loss_concept = criterion_concept(predicted_concepts, torch.argmax(concepts, dim=1))\n",
    "                val_total_loss += loss_concept.item()\n",
    "                val_concept_correct += (predicted_concepts.argmax(dim=1) == torch.argmax(concepts, dim=1)).sum().item()\n",
    "                val_total += concepts.size(0)\n",
    "\n",
    "        val_concept_accuracy = 100 * val_concept_correct / val_total\n",
    "\n",
    "        print(f\"Validation Concept Loss: {val_total_loss / len(val_loader):.4f}, Validation Concept Accuracy: {val_concept_accuracy:.2f}%\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_total_loss < best_val_loss:\n",
    "            best_val_loss = val_total_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_concept_model.pth\")\n",
    "            print(\"Best model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_separately_classification(model, train_loader, val_loader, device, optimizer_classification, epochs=10, patience=3):\n",
    "    criterion_classification = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        print(\"\\n-----------------------------------\")\n",
    "        progress_bar = tqdm(\n",
    "            desc=f\"[Classification Training: Epoch {epoch+1}/{epochs}]\",\n",
    "            total=len(train_loader),\n",
    "            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed}, it/s: {rate_fmt}]\",\n",
    "            ncols=100,\n",
    "            dynamic_ncols=True,\n",
    "            position=0,\n",
    "            leave=True,\n",
    "            mininterval=1\n",
    "        )\n",
    "\n",
    "        for images, labels, concepts in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer_classification.zero_grad()\n",
    "            _, predictions = model(images)\n",
    "\n",
    "            loss_classification = criterion_classification(predictions, labels)\n",
    "            loss_classification.backward()\n",
    "            optimizer_classification.step()\n",
    "\n",
    "            train_total_loss += loss_classification.item()\n",
    "            train_correct += (predictions.argmax(dim=1) == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'Train Loss': train_total_loss / (len(train_loader) * train_loader.batch_size),\n",
    "                                       'Train Accuracy': 100 * train_correct / train_total})\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        print(f\"Train Classification Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"Training Classification Loss: {train_total_loss / len(train_loader):.4f} \\t Training Classification Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_total_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, concepts in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                _, predictions = model(images)\n",
    "                loss_classification = criterion_classification(predictions, labels)\n",
    "                val_total_loss += loss_classification.item()\n",
    "                val_correct += (predictions.argmax(dim=1) == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss = val_total_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f\"Validation Classification Loss: {val_loss:.4f} \\t Validation Classification Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            print(\"Validation loss improved, saving model...\")\n",
    "            torch.save(model.state_dict(), 'best_classification_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    criterion_concept = nn.CrossEntropyLoss()\n",
    "    criterion_classification = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_samples = 0\n",
    "    correct_classifications = 0\n",
    "    correct_concepts = 0\n",
    "    total_concept_loss = 0.0\n",
    "    total_classification_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, concepts in test_loader:\n",
    "            images, labels, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "\n",
    "            predicted_concepts, predictions = model(images)\n",
    "\n",
    "            loss_concept = criterion_concept(predicted_concepts, torch.argmax(concepts, dim=1))\n",
    "            loss_classification = criterion_classification(predictions, labels)\n",
    "\n",
    "            total_concept_loss += loss_concept.item()\n",
    "            total_classification_loss += loss_classification.item()\n",
    "\n",
    "            total_samples += labels.size(0)\n",
    "            correct_classifications += (predictions.argmax(dim=1) == labels).sum().item()\n",
    "            correct_concepts += (predicted_concepts.argmax(dim=1) == torch.argmax(concepts, dim=1)).sum().item()\n",
    "\n",
    "    concept_accuracy = 100 * correct_concepts / total_samples\n",
    "    classification_accuracy = 100 * correct_classifications / total_samples\n",
    "    avg_concept_loss = total_concept_loss / len(test_loader)\n",
    "    avg_classification_loss = total_classification_loss / len(test_loader)\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(\"-------------------\")\n",
    "    print(f\"Concept Loss:          {avg_concept_loss:.4f} \\t Concept Accuracy:         {concept_accuracy:.2f}%\")\n",
    "    print(f\"Classification Loss:   {avg_classification_loss:.4f} \\t Classification Accuracy:  {classification_accuracy:.2f}%\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "\n",
    "    return concept_accuracy, classification_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data_loaders(PREPROCESSED_DIR, CSV_CONCEPTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_concepts = len(pd.get_dummies(pd.read_csv(CSV_CONCEPTS_PATH)[[\"cap_color\", \"cap_shape\", \"cap_texture\", \"ring_present\"]]).columns)\n",
    "num_classes = len(pd.read_csv(CSV_CONCEPTS_PATH)['Mushroom'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ConceptBottleneckModel(num_concepts, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_concept = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "optimizer_classification = torch.optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[Concept Training: Epoch 1/15]:   0%|          | 0/52 [elapsed: 00:00, it/s: ?it/s]\n",
      "[Concept Training: Epoch 1/15]:   0%|          | 0/52 [elapsed: 00:02, it/s: ?it/s]\n",
      "[Concept Training: Epoch 1/15]:   2%|▏         | 1/52 [elapsed: 00:02, it/s:  2.04s/it]\n",
      "[Concept Training: Epoch 1/15]:   2%|▏         | 1/52 [elapsed: 00:04, it/s:  2.04s/it]\n",
      "[Concept Training: Epoch 1/15]:   4%|▍         | 2/52 [elapsed: 00:04, it/s:  2.27s/it]\n",
      "[Concept Training: Epoch 1/15]:   4%|▍         | 2/52 [elapsed: 00:06, it/s:  2.27s/it]\n",
      "[Concept Training: Epoch 1/15]:   6%|▌         | 3/52 [elapsed: 00:06, it/s:  2.30s/it]\n",
      "[Concept Training: Epoch 1/15]:   6%|▌         | 3/52 [elapsed: 00:09, it/s:  2.30s/it]\n",
      "[Concept Training: Epoch 1/15]:   8%|▊         | 4/52 [elapsed: 00:09, it/s:  2.34s/it]\n",
      "[Concept Training: Epoch 1/15]:   8%|▊         | 4/52 [elapsed: 00:11, it/s:  2.34s/it]\n",
      "[Concept Training: Epoch 1/15]:  10%|▉         | 5/52 [elapsed: 00:11, it/s:  2.33s/it]\n",
      "[Concept Training: Epoch 1/15]:  10%|▉         | 5/52 [elapsed: 00:13, it/s:  2.33s/it]\n",
      "[Concept Training: Epoch 1/15]:  12%|█▏        | 6/52 [elapsed: 00:13, it/s:  2.26s/it]\n",
      "[Concept Training: Epoch 1/15]:  12%|█▏        | 6/52 [elapsed: 00:15, it/s:  2.26s/it]\n",
      "[Concept Training: Epoch 1/15]:  13%|█▎        | 7/52 [elapsed: 00:15, it/s:  2.22s/it]\n",
      "[Concept Training: Epoch 1/15]:  13%|█▎        | 7/52 [elapsed: 00:17, it/s:  2.22s/it]\n",
      "[Concept Training: Epoch 1/15]:  15%|█▌        | 8/52 [elapsed: 00:17, it/s:  2.19s/it]\n",
      "[Concept Training: Epoch 1/15]:  15%|█▌        | 8/52 [elapsed: 00:20, it/s:  2.19s/it]\n",
      "[Concept Training: Epoch 1/15]:  17%|█▋        | 9/52 [elapsed: 00:20, it/s:  2.18s/it]\n",
      "[Concept Training: Epoch 1/15]:  17%|█▋        | 9/52 [elapsed: 00:22, it/s:  2.18s/it]\n",
      "[Concept Training: Epoch 1/15]:  19%|█▉        | 10/52 [elapsed: 00:22, it/s:  2.17s/it]\n",
      "[Concept Training: Epoch 1/15]:  19%|█▉        | 10/52 [elapsed: 00:24, it/s:  2.17s/it]\n",
      "[Concept Training: Epoch 1/15]:  21%|██        | 11/52 [elapsed: 00:24, it/s:  2.19s/it]\n",
      "[Concept Training: Epoch 1/15]:  21%|██        | 11/52 [elapsed: 00:26, it/s:  2.19s/it]\n",
      "[Concept Training: Epoch 1/15]:  23%|██▎       | 12/52 [elapsed: 00:26, it/s:  2.17s/it]\n",
      "[Concept Training: Epoch 1/15]:  23%|██▎       | 12/52 [elapsed: 00:28, it/s:  2.17s/it]\n",
      "[Concept Training: Epoch 1/15]:  25%|██▌       | 13/52 [elapsed: 00:28, it/s:  2.21s/it]\n",
      "[Concept Training: Epoch 1/15]:  25%|██▌       | 13/52 [elapsed: 00:31, it/s:  2.21s/it]\n",
      "[Concept Training: Epoch 1/15]:  27%|██▋       | 14/52 [elapsed: 00:31, it/s:  2.21s/it]\n",
      "[Concept Training: Epoch 1/15]:  27%|██▋       | 14/52 [elapsed: 00:33, it/s:  2.21s/it]\n",
      "[Concept Training: Epoch 1/15]:  29%|██▉       | 15/52 [elapsed: 00:33, it/s:  2.24s/it]\n",
      "[Concept Training: Epoch 1/15]:  29%|██▉       | 15/52 [elapsed: 00:35, it/s:  2.24s/it]\n",
      "[Concept Training: Epoch 1/15]:  31%|███       | 16/52 [elapsed: 00:35, it/s:  2.23s/it]\n",
      "[Concept Training: Epoch 1/15]:  31%|███       | 16/52 [elapsed: 00:39, it/s:  2.23s/it]\n",
      "[Concept Training: Epoch 1/15]:  33%|███▎      | 17/52 [elapsed: 00:39, it/s:  2.65s/it]\n",
      "[Concept Training: Epoch 1/15]:  33%|███▎      | 17/52 [elapsed: 00:42, it/s:  2.65s/it]\n",
      "[Concept Training: Epoch 1/15]:  35%|███▍      | 18/52 [elapsed: 00:42, it/s:  2.96s/it]\n",
      "[Concept Training: Epoch 1/15]:  35%|███▍      | 18/52 [elapsed: 00:47, it/s:  2.96s/it]\n",
      "[Concept Training: Epoch 1/15]:  37%|███▋      | 19/52 [elapsed: 00:47, it/s:  3.50s/it]\n",
      "[Concept Training: Epoch 1/15]:  37%|███▋      | 19/52 [elapsed: 00:51, it/s:  3.50s/it]\n",
      "[Concept Training: Epoch 1/15]:  38%|███▊      | 20/52 [elapsed: 00:51, it/s:  3.69s/it]\n",
      "[Concept Training: Epoch 1/15]:  38%|███▊      | 20/52 [elapsed: 00:55, it/s:  3.69s/it]\n",
      "[Concept Training: Epoch 1/15]:  40%|████      | 21/52 [elapsed: 00:55, it/s:  3.65s/it]\n",
      "[Concept Training: Epoch 1/15]:  40%|████      | 21/52 [elapsed: 01:03, it/s:  3.65s/it]\n",
      "[Concept Training: Epoch 1/15]:  42%|████▏     | 22/52 [elapsed: 01:03, it/s:  5.12s/it]\n",
      "[Concept Training: Epoch 1/15]:  42%|████▏     | 22/52 [elapsed: 01:08, it/s:  5.12s/it]\n",
      "[Concept Training: Epoch 1/15]:  44%|████▍     | 23/52 [elapsed: 01:08, it/s:  4.82s/it]\n",
      "[Concept Training: Epoch 1/15]:  44%|████▍     | 23/52 [elapsed: 01:15, it/s:  4.82s/it]\n",
      "[Concept Training: Epoch 1/15]:  46%|████▌     | 24/52 [elapsed: 01:15, it/s:  5.57s/it]\n",
      "[Concept Training: Epoch 1/15]:  46%|████▌     | 24/52 [elapsed: 01:25, it/s:  5.57s/it]\n",
      "[Concept Training: Epoch 1/15]:  48%|████▊     | 25/52 [elapsed: 01:25, it/s:  6.92s/it]\n",
      "[Concept Training: Epoch 1/15]:  48%|████▊     | 25/52 [elapsed: 01:37, it/s:  6.92s/it]\n",
      "[Concept Training: Epoch 1/15]:  50%|█████     | 26/52 [elapsed: 01:37, it/s:  8.35s/it]\n",
      "[Concept Training: Epoch 1/15]:  50%|█████     | 26/52 [elapsed: 01:45, it/s:  8.35s/it]\n",
      "[Concept Training: Epoch 1/15]:  52%|█████▏    | 27/52 [elapsed: 01:45, it/s:  8.43s/it]\n",
      "[Concept Training: Epoch 1/15]:  52%|█████▏    | 27/52 [elapsed: 01:55, it/s:  8.43s/it]\n",
      "[Concept Training: Epoch 1/15]:  54%|█████▍    | 28/52 [elapsed: 01:55, it/s:  8.73s/it]\n",
      "[Concept Training: Epoch 1/15]:  54%|█████▍    | 28/52 [elapsed: 02:00, it/s:  8.73s/it]\n",
      "[Concept Training: Epoch 1/15]:  56%|█████▌    | 29/52 [elapsed: 02:00, it/s:  7.78s/it]\n",
      "[Concept Training: Epoch 1/15]:  56%|█████▌    | 29/52 [elapsed: 02:07, it/s:  7.78s/it]\n",
      "[Concept Training: Epoch 1/15]:  58%|█████▊    | 30/52 [elapsed: 02:07, it/s:  7.43s/it]\n",
      "[Concept Training: Epoch 1/15]:  58%|█████▊    | 30/52 [elapsed: 02:13, it/s:  7.43s/it]\n",
      "[Concept Training: Epoch 1/15]:  60%|█████▉    | 31/52 [elapsed: 02:13, it/s:  6.94s/it]\n",
      "[Concept Training: Epoch 1/15]:  60%|█████▉    | 31/52 [elapsed: 02:23, it/s:  6.94s/it]\n",
      "[Concept Training: Epoch 1/15]:  62%|██████▏   | 32/52 [elapsed: 02:23, it/s:  7.89s/it]\n",
      "[Concept Training: Epoch 1/15]:  62%|██████▏   | 32/52 [elapsed: 02:31, it/s:  7.89s/it]\n",
      "[Concept Training: Epoch 1/15]:  63%|██████▎   | 33/52 [elapsed: 02:31, it/s:  8.04s/it]\n",
      "[Concept Training: Epoch 1/15]:  63%|██████▎   | 33/52 [elapsed: 02:33, it/s:  8.04s/it]\n",
      "[Concept Training: Epoch 1/15]:  65%|██████▌   | 34/52 [elapsed: 02:33, it/s:  6.32s/it]\n",
      "[Concept Training: Epoch 1/15]:  65%|██████▌   | 34/52 [elapsed: 02:36, it/s:  6.32s/it]\n",
      "[Concept Training: Epoch 1/15]:  67%|██████▋   | 35/52 [elapsed: 02:36, it/s:  5.23s/it]\n",
      "[Concept Training: Epoch 1/15]:  67%|██████▋   | 35/52 [elapsed: 02:38, it/s:  5.23s/it]\n",
      "[Concept Training: Epoch 1/15]:  69%|██████▉   | 36/52 [elapsed: 02:38, it/s:  4.31s/it]\n",
      "[Concept Training: Epoch 1/15]:  69%|██████▉   | 36/52 [elapsed: 02:40, it/s:  4.31s/it]\n",
      "[Concept Training: Epoch 1/15]:  71%|███████   | 37/52 [elapsed: 02:40, it/s:  3.65s/it]\n",
      "[Concept Training: Epoch 1/15]:  71%|███████   | 37/52 [elapsed: 02:42, it/s:  3.65s/it]\n",
      "[Concept Training: Epoch 1/15]:  73%|███████▎  | 38/52 [elapsed: 02:42, it/s:  3.15s/it]\n",
      "[Concept Training: Epoch 1/15]:  73%|███████▎  | 38/52 [elapsed: 02:44, it/s:  3.15s/it]\n",
      "[Concept Training: Epoch 1/15]:  75%|███████▌  | 39/52 [elapsed: 02:44, it/s:  2.81s/it]\n",
      "[Concept Training: Epoch 1/15]:  75%|███████▌  | 39/52 [elapsed: 02:46, it/s:  2.81s/it]\n",
      "[Concept Training: Epoch 1/15]:  77%|███████▋  | 40/52 [elapsed: 02:46, it/s:  2.54s/it]\n",
      "[Concept Training: Epoch 1/15]:  77%|███████▋  | 40/52 [elapsed: 02:48, it/s:  2.54s/it]\n",
      "[Concept Training: Epoch 1/15]:  79%|███████▉  | 41/52 [elapsed: 02:48, it/s:  2.40s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_separately_concept\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_concept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_concept\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 25\u001b[0m, in \u001b[0;36mtrain_separately_concept\u001b[1;34m(model, train_loader, val_loader, device, optimizer_concept, epochs, patience)\u001b[0m\n\u001b[0;32m     12\u001b[0m train_total \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     14\u001b[0m progress_bar \u001b[38;5;241m=\u001b[39m tqdm(\n\u001b[0;32m     15\u001b[0m     desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m[Concept Training: Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     16\u001b[0m     total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     22\u001b[0m     mininterval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     23\u001b[0m )\n\u001b[1;32m---> 25\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcepts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcepts\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconcepts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer_concept\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:398\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    338\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 398\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:212\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    208\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 212\u001b[0m         \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    213\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed\n\u001b[0;32m    214\u001b[0m     ]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:155\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    153\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    154\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 155\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    157\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    158\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:272\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    270\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    271\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_separately_concept(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    device=device, \n",
    "    optimizer_concept=optimizer_concept, \n",
    "    epochs=15, \n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_separately_classification(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    device=device, \n",
    "    optimizer_classification=optimizer_classification, \n",
    "    epochs=15, \n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "concept_acc, classification_acc = evaluate_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Concept Accuracy: {concept_acc:.2f}%\")\n",
    "print(f\"Classification Accuracy: {classification_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intervention Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(tensor, mean, std):\n",
    "    return tensor.mul_(torch.tensor(std).view(-1, 1, 1)).add_(torch.tensor(mean).view(-1, 1, 1))\n",
    "\n",
    "\n",
    "def get_random_image_data(test_loader, device):\n",
    "    image, label, concepts = test_loader.dataset[random.randint(0, len(test_loader.dataset) - 1)]\n",
    "    return (\n",
    "        image.unsqueeze(0).to(device), \n",
    "        torch.tensor([label], device=device), \n",
    "        concepts.unsqueeze(0).to(device) if isinstance(concepts, torch.Tensor) else torch.tensor(concepts, device=device).unsqueeze(0)\n",
    "    )\n",
    "\n",
    "\n",
    "def display_concepts(concepts):\n",
    "    idx, concept_values = 0, []\n",
    "    for concept in CONCEPT_LABELS:\n",
    "        selected_idx = concepts[0, idx:idx + len(CONCEPT_VALUES[concept])].argmax().item()\n",
    "        concept_values.append(f\"{concept}: {CONCEPT_VALUES[concept][selected_idx]}\")\n",
    "        idx += len(CONCEPT_VALUES[concept])\n",
    "    return concept_values\n",
    "\n",
    "\n",
    "def display_class_and_concepts(label, concepts, is_predicted=False):\n",
    "    print('----------------------------------')\n",
    "    print(f\"{'Predicted' if is_predicted else 'True'} class: {CLASS_NAMES[label.item()]}\")\n",
    "    for concept in display_concepts(concepts):\n",
    "        print(f'- {concept}')\n",
    "\n",
    "\n",
    "def predict_with_model(model, image, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        concepts, logits = model(image.to(device))\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "        confidence, predicted_class_idx = probabilities.max(1)\n",
    "    return concepts, predicted_class_idx, confidence.item()\n",
    "\n",
    "\n",
    "def show_true_and_predicted(test_loader, model, device):\n",
    "    tensor, true_label, true_concepts = get_random_image_data(test_loader, device)\n",
    "    image = unnormalize(tensor.squeeze(), [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]).cpu().numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    predicted_concepts, predicted_class_idx, confidence = predict_with_model(model, tensor, device)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"True class and concepts:\")\n",
    "    display_class_and_concepts(true_label, true_concepts)\n",
    "    \n",
    "    print(\"\\n\\nPredicted class and concepts:\")\n",
    "    display_class_and_concepts(predicted_class_idx, predicted_concepts, True)\n",
    "    print(f\"\\nConfidence: {confidence:.2%}\")\n",
    "\n",
    "    return predicted_concepts, predicted_class_idx, true_label, image, confidence\n",
    "\n",
    "\n",
    "def intervene_on_concept(concepts, concept_name, new_value):\n",
    "    start_idx = sum(len(CONCEPT_VALUES[label]) for label in CONCEPT_LABELS[:CONCEPT_LABELS.index(concept_name)])\n",
    "    concepts[0, start_idx:start_idx + len(CONCEPT_VALUES[concept_name])] = 0\n",
    "    concepts[0, start_idx + CONCEPT_VALUES[concept_name].index(new_value)] = 1\n",
    "    return concepts\n",
    "\n",
    "\n",
    "def intervene_on_multiple_concepts(concepts, interventions):\n",
    "    for concept_name, new_value in interventions.items():\n",
    "        concepts = intervene_on_concept(concepts.clone(), concept_name, new_value)\n",
    "    return concepts\n",
    "\n",
    "\n",
    "def intervene_and_predict(model, modified_concepts, image, device):\n",
    "    logits = model.classifier(modified_concepts)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    confidence, final_class_idx = probabilities.max(1)\n",
    "    return CLASS_NAMES[final_class_idx.item()], modified_concepts, confidence.item()\n",
    "\n",
    "\n",
    "def apply_intervention(predicted_concepts, corrections, model, image, device):\n",
    "    modified_concepts = intervene_on_multiple_concepts(predicted_concepts, corrections)\n",
    "    final_class, final_concepts, confidence_after = intervene_and_predict(model, modified_concepts, image, device)\n",
    "    final_class_idx = torch.tensor([k for k, v in CLASS_NAMES.items() if v == final_class][0])\n",
    "\n",
    "    print(\"\\n\\nAfter Intervention:\")\n",
    "    display_class_and_concepts(final_class_idx, final_concepts, True)\n",
    "    print(f\"\\nConfidence: {confidence_after:.2%}\")\n",
    "\n",
    "    return final_concepts, final_class_idx, confidence_after\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_concepts, predicted_class_idx, true_label, image, confidence_before = show_true_and_predicted(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = {'cap_color': 'brown', 'cap_shape': 'bulbous', 'cap_texture': 'smooth', 'ring_present': 'yes'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_concepts, final_class_idx, confidence_after = apply_intervention(predicted_concepts, corrections, model, image, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
