{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR = Path.cwd()\n",
    "ROOT_DIR = SRC_DIR.parent\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'dataset')\n",
    "PREPROCESSED_DIR = os.path.join(DATA_DIR, 'preprocessed')\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "CBM_DIR = os.path.join(MODEL_DIR, 'cbm')\n",
    "os.makedirs(CBM_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = os.path.join(DATA_DIR, 'csv_mappings', 'train.csv')\n",
    "CSV_CONCEPTS_PATH = os.path.join(DATA_DIR, 'csv_mappings', 'train_concepts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NAMES = {\n",
    "    0: \"amanita\",\n",
    "    1: \"boletus\",\n",
    "    2: \"chantelle\",\n",
    "    3: \"deterrimus\",\n",
    "    4: \"rufus\",\n",
    "    5: \"torminosus\",\n",
    "    6: \"aurantiacum\",\n",
    "    7: \"procera\",\n",
    "    8: \"involutus\",\n",
    "    9: \"russula\"\n",
    "}\n",
    "\n",
    "CONCEPT_LABELS = [\"cap_color\", \"cap_shape\", \"cap_texture\", \"ring_present\"]\n",
    "\n",
    "CONCEPT_VALUES = {\n",
    "    \"cap_color\": [\"red\", \"brown\", \"yellow\", \"white\"],\n",
    "    \"cap_shape\": [\"convex\", \"flat\", \"bulbous\", \"inverted\"],\n",
    "    \"cap_texture\": [\"scaly\", \"smooth\", \"warty\"],\n",
    "    \"ring_present\": [\"yes\", \"no\"]\n",
    "}\n",
    "\n",
    "CONCEPT_MAPPING = {\n",
    "    0: {\"cap_color\": \"red\", \"cap_shape\": \"flat\", \"cap_texture\": \"warty\", \"ring_present\": \"yes\"},            # Amanita \n",
    "    1: {\"cap_color\": \"brown\", \"cap_shape\": \"convex\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},        # Boletus \n",
    "    2: {\"cap_color\": \"yellow\", \"cap_shape\": \"inverted\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},     # Chantelle\n",
    "    3: {\"cap_color\": \"red\", \"cap_shape\": \"flat\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},            # Deterrimus \n",
    "    4: {\"cap_color\": \"brown\", \"cap_shape\": \"inverted\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},      # Rufus \n",
    "    5: {\"cap_color\": \"white\", \"cap_shape\": \"inverted\", \"cap_texture\": \"scaly\", \"ring_present\": \"no\"},       # Torminosus \n",
    "    6: {\"cap_color\": \"brown\", \"cap_shape\": \"convex\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},        # Aurantiacum\n",
    "    7: {\"cap_color\": \"white\", \"cap_shape\": \"bulbous\", \"cap_texture\": \"scaly\", \"ring_present\": \"yes\"},       # Procera\n",
    "    8: {\"cap_color\": \"brown\", \"cap_shape\": \"convex\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},        # Involutus \n",
    "    9: {\"cap_color\": \"red\", \"cap_shape\": \"flat\", \"cap_texture\": \"smooth\", \"ring_present\": \"no\"},            # Russula \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_concept_labels(mushroom_class):\n",
    "    concepts = CONCEPT_MAPPING.get(mushroom_class, {})\n",
    "    return concepts\n",
    "\n",
    "concepts_list = []\n",
    "for index, row in df.iterrows():\n",
    "    mushroom_class = row['Mushroom']\n",
    "    concepts = get_concept_labels(mushroom_class)\n",
    "    concepts_list.append(concepts)\n",
    "\n",
    "concept_df = pd.DataFrame(concepts_list)\n",
    "df_with_concepts = pd.concat([df, concept_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New CSV with concepts saved to: c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\dataset\\csv_mappings\\train_concepts.csv\n"
     ]
    }
   ],
   "source": [
    "df_with_concepts.to_csv(CSV_CONCEPTS_PATH, index=False)\n",
    "print(f\"New CSV with concepts saved to: {CSV_CONCEPTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptBottleneckModel(nn.Module):\n",
    "    def __init__(self, num_concepts, num_classes):\n",
    "        super(ConceptBottleneckModel, self).__init__()\n",
    "\n",
    "        self.feature_extractor = models.resnet50(pretrained=True)\n",
    "\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.feature_extractor.fc = nn.Linear(self.feature_extractor.fc.in_features, num_concepts)\n",
    "\n",
    "        for param in self.feature_extractor.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_concepts, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        concepts = self.feature_extractor(x)\n",
    "        predictions = self.classifier(concepts)\n",
    "        return concepts, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptMushroomDataset(Dataset):\n",
    "    def __init__(self, preprocessed_dir, csv_path, transform=None):\n",
    "        self.preprocessed_dir = preprocessed_dir\n",
    "        self.csv_data = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_ids = self.csv_data['Image'].values\n",
    "        self.labels = self.csv_data['Mushroom'].values\n",
    "\n",
    "        concept_columns = [\"cap_color\", \"cap_shape\", \"cap_texture\", \"ring_present\"]\n",
    "        self.concepts = pd.get_dummies(self.csv_data[concept_columns]).values.astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = str(self.image_ids[idx]).zfill(5)\n",
    "        label = self.labels[idx]\n",
    "        concepts = torch.tensor(self.concepts[idx], dtype=torch.float32)\n",
    "\n",
    "        image_path = os.path.join(self.preprocessed_dir, f\"{image_id}.pt\")\n",
    "        image = torch.load(image_path, weights_only=True)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(preprocessed_dir, csv_path, batch_size=32):\n",
    "    dataset = ConceptMushroomDataset(preprocessed_dir, csv_path)\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, temp_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "    val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    val_subset = Subset(dataset, val_indices)\n",
    "    test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_concept_accuracy(predicted_concepts, ground_truth):\n",
    "    predicted_classes = torch.argmax(predicted_concepts, dim=1)\n",
    "    true_classes = torch.argmax(ground_truth, dim=1)\n",
    "    correct = (predicted_classes == true_classes).sum().item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_separately_concept(model, train_loader, val_loader, device, optimizer_concept, epochs=10, patience=3):\n",
    "    criterion_concept = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0.0\n",
    "        train_concept_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        progress_bar = tqdm(\n",
    "            desc=f\"[Concept Training: Epoch {epoch+1}/{epochs}]\",\n",
    "            total=len(train_loader),\n",
    "            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed}, it/s: {rate_fmt}]\",\n",
    "            ncols=100,\n",
    "            dynamic_ncols=True,\n",
    "            position=0,\n",
    "            leave=True,\n",
    "            mininterval=1\n",
    "        )\n",
    "\n",
    "        for images, labels, concepts in train_loader:\n",
    "            images, concepts = images.to(device), concepts.to(device)\n",
    "\n",
    "            optimizer_concept.zero_grad()\n",
    "            predicted_concepts, _ = model(images)\n",
    "\n",
    "            # Compute concept loss\n",
    "            loss_concept = criterion_concept(predicted_concepts, torch.argmax(concepts, dim=1))\n",
    "            loss_concept.backward()\n",
    "            optimizer_concept.step()\n",
    "\n",
    "            train_total_loss += loss_concept.item()\n",
    "            train_concept_correct += (predicted_concepts.argmax(dim=1) == torch.argmax(concepts, dim=1)).sum().item()\n",
    "            train_total += concepts.size(0)\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'Train Loss': train_total_loss / (len(train_loader) * train_loader.batch_size),\n",
    "                                       'Train Accuracy': 100 * train_concept_correct / train_total})\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        train_concept_accuracy = 100 * train_concept_correct / train_total\n",
    "\n",
    "        print(f\"Train Concept Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"Training Concept Loss: {train_total_loss / len(train_loader):.4f}, Training Concept Accuracy: {train_concept_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_total_loss = 0.0\n",
    "        val_concept_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, concepts in val_loader:\n",
    "                images, concepts = images.to(device), concepts.to(device)\n",
    "\n",
    "                predicted_concepts, _ = model(images)\n",
    "\n",
    "                loss_concept = criterion_concept(predicted_concepts, torch.argmax(concepts, dim=1))\n",
    "                val_total_loss += loss_concept.item()\n",
    "                val_concept_correct += (predicted_concepts.argmax(dim=1) == torch.argmax(concepts, dim=1)).sum().item()\n",
    "                val_total += concepts.size(0)\n",
    "\n",
    "        val_concept_accuracy = 100 * val_concept_correct / val_total\n",
    "\n",
    "        print(f\"Validation Concept Loss: {val_total_loss / len(val_loader):.4f}, Validation Concept Accuracy: {val_concept_accuracy:.2f}%\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_total_loss < best_val_loss:\n",
    "            best_val_loss = val_total_loss\n",
    "            patience_counter = 0\n",
    "            torch.save(model.state_dict(), \"best_concept_model.pth\")\n",
    "            print(\"Best model saved.\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered.\")\n",
    "                break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_separately_classification(model, train_loader, val_loader, device, optimizer_classification, epochs=10, patience=3):\n",
    "    criterion_classification = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        progress_bar = tqdm(\n",
    "            desc=f\"[Classification Training: Epoch {epoch+1}/{epochs}]\",\n",
    "            total=len(train_loader),\n",
    "            bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [elapsed: {elapsed}, it/s: {rate_fmt}]\",\n",
    "            ncols=100,\n",
    "            dynamic_ncols=True,\n",
    "            position=0,\n",
    "            leave=True,\n",
    "            mininterval=1\n",
    "        )\n",
    "\n",
    "        for images, labels, concepts in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer_classification.zero_grad()\n",
    "            _, predictions = model(images)\n",
    "\n",
    "            loss_classification = criterion_classification(predictions, labels)\n",
    "            loss_classification.backward()\n",
    "            optimizer_classification.step()\n",
    "\n",
    "            train_total_loss += loss_classification.item()\n",
    "            train_correct += (predictions.argmax(dim=1) == labels).sum().item()\n",
    "            train_total += labels.size(0)\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix({'Train Loss': train_total_loss / (len(train_loader) * train_loader.batch_size),\n",
    "                                       'Train Accuracy': 100 * train_correct / train_total})\n",
    "            progress_bar.update(1)\n",
    "\n",
    "        progress_bar.close()\n",
    "\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        print(f\"Train Classification Epoch {epoch+1}/{epochs}:\")\n",
    "        print(f\"Training Classification Loss: {train_total_loss / len(train_loader):.4f} \\t Training Classification Accuracy: {train_accuracy:.2f}%\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_total_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, concepts in val_loader:\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                _, predictions = model(images)\n",
    "                loss_classification = criterion_classification(predictions, labels)\n",
    "                val_total_loss += loss_classification.item()\n",
    "                val_correct += (predictions.argmax(dim=1) == labels).sum().item()\n",
    "                val_total += labels.size(0)\n",
    "\n",
    "        val_loss = val_total_loss / len(val_loader)\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        print(f\"Validation Classification Loss: {val_loss:.4f} \\t Validation Classification Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "        # Early stopping check\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            print(\"Validation loss improved, saving model...\")\n",
    "            torch.save(model.state_dict(), 'best_classification_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            print(f\"No improvement in validation loss. Patience counter: {patience_counter}/{patience}\")\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    criterion_concept = nn.CrossEntropyLoss()\n",
    "    criterion_classification = nn.CrossEntropyLoss()\n",
    "\n",
    "    total_samples = 0\n",
    "    correct_classifications = 0\n",
    "    correct_concepts = 0\n",
    "    total_concept_loss = 0.0\n",
    "    total_classification_loss = 0.0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for images, labels, concepts in test_loader:\n",
    "            images, labels, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "\n",
    "            predicted_concepts, predictions = model(images)\n",
    "\n",
    "            loss_concept = criterion_concept(predicted_concepts, torch.argmax(concepts, dim=1))\n",
    "            loss_classification = criterion_classification(predictions, labels)\n",
    "\n",
    "            total_concept_loss += loss_concept.item()\n",
    "            total_classification_loss += loss_classification.item()\n",
    "\n",
    "            total_samples += labels.size(0)\n",
    "            correct_classifications += (predictions.argmax(dim=1) == labels).sum().item()\n",
    "            correct_concepts += (predicted_concepts.argmax(dim=1) == torch.argmax(concepts, dim=1)).sum().item()\n",
    "\n",
    "    concept_accuracy = 100 * correct_concepts / total_samples\n",
    "    classification_accuracy = 100 * correct_classifications / total_samples\n",
    "    avg_concept_loss = total_concept_loss / len(test_loader)\n",
    "    avg_classification_loss = total_classification_loss / len(test_loader)\n",
    "\n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(\"-------------------\")\n",
    "    print(f\"Concept Loss:          {avg_concept_loss:.4f} \\t Concept Accuracy:         {concept_accuracy:.2f}%\")\n",
    "    print(f\"Classification Loss:   {avg_classification_loss:.4f} \\t Classification Accuracy:  {classification_accuracy:.2f}%\")\n",
    "    print(\"---------------------------------------------------------\")\n",
    "\n",
    "    return concept_accuracy, classification_accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data_loaders(PREPROCESSED_DIR, CSV_CONCEPTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_concepts = len(pd.get_dummies(pd.read_csv(CSV_CONCEPTS_PATH)[[\"cap_color\", \"cap_shape\", \"cap_texture\", \"ring_present\"]]).columns)\n",
    "num_classes = len(pd.read_csv(CSV_CONCEPTS_PATH)['Mushroom'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = ConceptBottleneckModel(num_concepts, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seperate training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_concept = torch.optim.Adam(model.parameters(), lr=0.0005)\n",
    "optimizer_classification = torch.optim.Adam(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Concept Training: Epoch 1/15]:  46%|████▌     | 24/52 [elapsed: 00:49, it/s:  2.11s/it]"
     ]
    }
   ],
   "source": [
    "train_separately_concept(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    device=device, \n",
    "    optimizer_concept=optimizer_concept, \n",
    "    epochs=15, \n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_separately_classification(\n",
    "    model=model, \n",
    "    train_loader=train_loader, \n",
    "    val_loader=val_loader, \n",
    "    device=device, \n",
    "    optimizer_classification=optimizer_classification, \n",
    "    epochs=15, \n",
    "    patience=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "concept_acc, classification_acc = evaluate_model(model, test_loader, device)\n",
    "\n",
    "print(f\"Concept Accuracy: {concept_acc:.2f}%\")\n",
    "print(f\"Classification Accuracy: {classification_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intervention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intervention Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unnormalize(tensor, mean, std):\n",
    "    return tensor.mul_(torch.tensor(std).view(-1, 1, 1)).add_(torch.tensor(mean).view(-1, 1, 1))\n",
    "\n",
    "\n",
    "def get_random_image_data(test_loader, device):\n",
    "    image, label, concepts = test_loader.dataset[random.randint(0, len(test_loader.dataset) - 1)]\n",
    "    return (\n",
    "        image.unsqueeze(0).to(device), \n",
    "        torch.tensor([label], device=device), \n",
    "        concepts.unsqueeze(0).to(device) if isinstance(concepts, torch.Tensor) else torch.tensor(concepts, device=device).unsqueeze(0)\n",
    "    )\n",
    "\n",
    "\n",
    "def display_concepts(concepts):\n",
    "    idx, concept_values = 0, []\n",
    "    for concept in CONCEPT_LABELS:\n",
    "        selected_idx = concepts[0, idx:idx + len(CONCEPT_VALUES[concept])].argmax().item()\n",
    "        concept_values.append(f\"{concept}: {CONCEPT_VALUES[concept][selected_idx]}\")\n",
    "        idx += len(CONCEPT_VALUES[concept])\n",
    "    return concept_values\n",
    "\n",
    "\n",
    "def display_class_and_concepts(label, concepts, is_predicted=False):\n",
    "    print('----------------------------------')\n",
    "    print(f\"{'Predicted' if is_predicted else 'True'} class: {CLASS_NAMES[label.item()]}\")\n",
    "    for concept in display_concepts(concepts):\n",
    "        print(f'- {concept}')\n",
    "\n",
    "\n",
    "def predict_with_model(model, image, device):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        concepts, logits = model(image.to(device))\n",
    "        probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "        confidence, predicted_class_idx = probabilities.max(1)\n",
    "    return concepts, predicted_class_idx, confidence.item()\n",
    "\n",
    "\n",
    "def show_true_and_predicted(test_loader, model, device):\n",
    "    tensor, true_label, true_concepts = get_random_image_data(test_loader, device)\n",
    "    image = unnormalize(tensor.squeeze(), [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]).cpu().numpy().transpose((1, 2, 0))\n",
    "    \n",
    "    predicted_concepts, predicted_class_idx, confidence = predict_with_model(model, tensor, device)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"True class and concepts:\")\n",
    "    display_class_and_concepts(true_label, true_concepts)\n",
    "    \n",
    "    print(\"\\n\\nPredicted class and concepts:\")\n",
    "    display_class_and_concepts(predicted_class_idx, predicted_concepts, True)\n",
    "    print(f\"\\nConfidence: {confidence:.2%}\")\n",
    "\n",
    "    return predicted_concepts, predicted_class_idx, true_label, image, confidence\n",
    "\n",
    "\n",
    "def intervene_on_concept(concepts, concept_name, new_value):\n",
    "    start_idx = sum(len(CONCEPT_VALUES[label]) for label in CONCEPT_LABELS[:CONCEPT_LABELS.index(concept_name)])\n",
    "    concepts[0, start_idx:start_idx + len(CONCEPT_VALUES[concept_name])] = 0\n",
    "    concepts[0, start_idx + CONCEPT_VALUES[concept_name].index(new_value)] = 1\n",
    "    return concepts\n",
    "\n",
    "\n",
    "def intervene_on_multiple_concepts(concepts, interventions):\n",
    "    for concept_name, new_value in interventions.items():\n",
    "        concepts = intervene_on_concept(concepts.clone(), concept_name, new_value)\n",
    "    return concepts\n",
    "\n",
    "\n",
    "def intervene_and_predict(model, modified_concepts, image, device):\n",
    "    logits = model.classifier(modified_concepts)\n",
    "    probabilities = torch.nn.functional.softmax(logits, dim=1)\n",
    "    confidence, final_class_idx = probabilities.max(1)\n",
    "    return CLASS_NAMES[final_class_idx.item()], modified_concepts, confidence.item()\n",
    "\n",
    "\n",
    "def apply_intervention(predicted_concepts, corrections, model, image, device):\n",
    "    modified_concepts = intervene_on_multiple_concepts(predicted_concepts, corrections)\n",
    "    final_class, final_concepts, confidence_after = intervene_and_predict(model, modified_concepts, image, device)\n",
    "    final_class_idx = torch.tensor([k for k, v in CLASS_NAMES.items() if v == final_class][0])\n",
    "\n",
    "    print(\"\\n\\nAfter Intervention:\")\n",
    "    display_class_and_concepts(final_class_idx, final_concepts, True)\n",
    "    print(f\"\\nConfidence: {confidence_after:.2%}\")\n",
    "\n",
    "    return final_concepts, final_class_idx, confidence_after\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_concepts, predicted_class_idx, true_label, image, confidence_before = show_true_and_predicted(test_loader, model, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrections = {'cap_color': 'brown', 'cap_shape': 'bulbous', 'cap_texture': 'smooth', 'ring_present': 'yes'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_concepts, final_class_idx, confidence_after = apply_intervention(predicted_concepts, corrections, model, image, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
