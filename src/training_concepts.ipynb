{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision import models, transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import ToPILImage\n",
    "import random\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC_DIR = Path.cwd()\n",
    "ROOT_DIR = SRC_DIR.parent\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'dataset')\n",
    "PREPROCESSED_DIR = os.path.join(DATA_DIR, 'preprocessed')\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, 'models')\n",
    "CBM_DIR = os.path.join(MODEL_DIR, 'cbm')\n",
    "os.makedirs(CBM_DIR, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_PATH = os.path.join(DATA_DIR, 'csv_mappings', 'train.csv')\n",
    "CSV_CONCEPTS_PATH = os.path.join(DATA_DIR, 'csv_mappings', 'train_concepts.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class mapping\n",
    "CLASS_NAMES = {\n",
    "    0: \"amanita\",\n",
    "    1: \"boletus\",\n",
    "    2: \"chantelle\",\n",
    "    3: \"deterrimus\",\n",
    "    4: \"rufus\",\n",
    "    5: \"torminosus\",\n",
    "    6: \"aurantiacum\",\n",
    "    7: \"procera\",\n",
    "    8: \"involutus\",\n",
    "    9: \"russula\"\n",
    "}\n",
    "\n",
    "# Concept mapping for each class\n",
    "CONCEPT_MAPPING = {\n",
    "    0: [\"red\", \"convex\", \"scaly\", \"yes\", \"thin\"],\n",
    "    1: [\"brown\", \"flat\", \"smooth\", \"no\", \"medium\"],\n",
    "    2: [\"yellow\", \"convex\", \"warty\", \"yes\", \"thick\"],\n",
    "    3: [\"white\", \"bulbous\", \"smooth\", \"no\", \"thin\"],\n",
    "    4: [\"brown\", \"inverted\", \"scaly\", \"yes\", \"medium\"],\n",
    "    5: [\"yellow\", \"flat\", \"smooth\", \"no\", \"thick\"],\n",
    "    6: [\"red\", \"convex\", \"warty\", \"yes\", \"thin\"],\n",
    "    7: [\"white\", \"flat\", \"smooth\", \"no\", \"medium\"],\n",
    "    8: [\"brown\", \"bulbous\", \"scaly\", \"yes\", \"thick\"],\n",
    "    9: [\"yellow\", \"inverted\", \"smooth\", \"no\", \"thin\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_columns = [\"cap_color\", \"cap_shape\", \"cap_texture\", \"ring_present\", \"stem_thickness\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "concept_rows = []\n",
    "for _, row in df.iterrows():\n",
    "    class_label = row[\"Mushroom\"]\n",
    "    concepts = CONCEPT_MAPPING[class_label]\n",
    "    concept_rows.append(concepts)\n",
    "\n",
    "concept_df = pd.DataFrame(concept_rows, columns=concept_columns)\n",
    "updated_df = pd.concat([df, concept_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated CSV saved to c:\\Users\\ilian\\Documents\\Projects\\git_projects\\university\\mushroom_classification\\dataset\\csv_mappings\\train_concepts.csv\n"
     ]
    }
   ],
   "source": [
    "updated_df.to_csv(CSV_CONCEPTS_PATH, index=False)\n",
    "print(f\"Updated CSV saved to {CSV_CONCEPTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classes & Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptBottleneckModel(nn.Module):\n",
    "    def __init__(self, num_concepts, num_classes):\n",
    "        super(ConceptBottleneckModel, self).__init__()\n",
    "\n",
    "        self.feature_extractor = models.resnet18(pretrained=True)\n",
    "\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.feature_extractor.fc = nn.Linear(self.feature_extractor.fc.in_features, num_concepts)\n",
    "\n",
    "        for param in self.feature_extractor.fc.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(num_concepts, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "        for param in self.classifier.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "    def forward(self, x):\n",
    "        concepts = self.feature_extractor(x)\n",
    "        predictions = self.classifier(concepts)\n",
    "        return concepts, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConceptMushroomDataset(Dataset):\n",
    "    def __init__(self, preprocessed_dir, csv_path, transform=None):\n",
    "        self.preprocessed_dir = preprocessed_dir\n",
    "        self.csv_data = pd.read_csv(csv_path)\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_ids = self.csv_data['Image'].values\n",
    "        self.labels = self.csv_data['Mushroom'].values\n",
    "\n",
    "        concept_columns = [\"cap_color\", \"cap_shape\", \"cap_texture\", \"ring_present\", \"stem_thickness\"]\n",
    "        self.concepts = pd.get_dummies(self.csv_data[concept_columns]).values.astype('float32')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = str(self.image_ids[idx]).zfill(5)\n",
    "        label = self.labels[idx]\n",
    "        concepts = torch.tensor(self.concepts[idx], dtype=torch.float32)\n",
    "\n",
    "        image_path = os.path.join(self.preprocessed_dir, f\"{image_id}.pt\")\n",
    "        image = torch.load(image_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, concepts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_data_loaders(preprocessed_dir, csv_path, batch_size=32):\n",
    "    dataset = ConceptMushroomDataset(preprocessed_dir, csv_path)\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    train_indices, temp_indices = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "    val_indices, test_indices = train_test_split(temp_indices, test_size=0.5, random_state=42)\n",
    "\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    val_subset = Subset(dataset, val_indices)\n",
    "    test_subset = Subset(dataset, test_indices)\n",
    "\n",
    "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_subset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_concept_accuracy(predicted_concepts, ground_truth):\n",
    "    predicted_classes = torch.argmax(predicted_concepts, dim=1)\n",
    "    true_classes = torch.argmax(ground_truth, dim=1)\n",
    "    correct = (predicted_classes == true_classes).sum().item()\n",
    "    return correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_jointly(model, train_loader, val_loader, device, optimizer, epochs=10, lambda_concept=0.5, lambda_classification=0.5):\n",
    "    criterion_concept = nn.CrossEntropyLoss()  # Use CrossEntropyLoss for categorical concepts\n",
    "    criterion_classification = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_total_loss = 0.0\n",
    "        train_concept_loss = 0.0\n",
    "        train_classification_loss = 0.0\n",
    "        train_total = 0\n",
    "        train_correct = 0\n",
    "        train_concept_correct = 0\n",
    "\n",
    "        for images, labels, concepts in tqdm(train_loader, desc=f\"[Joint Training: Epoch {epoch+1}/{epochs}]\"):\n",
    "            images, labels, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            predicted_concepts, predictions = model(images)\n",
    "\n",
    "            # Calculate concept loss (CrossEntropyLoss expects class indices)\n",
    "            loss_concept = criterion_concept(predicted_concepts, torch.argmax(concepts, dim=1))  # Use argmax on one-hot encoded concepts\n",
    "            loss_classification = criterion_classification(predictions, labels)\n",
    "            loss = lambda_concept * loss_concept + lambda_classification * loss_classification\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Track losses and accuracy\n",
    "            train_total_loss += loss.item()\n",
    "            train_concept_loss += loss_concept.item()\n",
    "            train_classification_loss += loss_classification.item()\n",
    "            \n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predictions.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            # Calculate concept accuracy\n",
    "            train_concept_correct += calculate_concept_accuracy(predicted_concepts, concepts)\n",
    "\n",
    "        # Calculate accuracies\n",
    "        train_accuracy = 100 * train_correct / train_total\n",
    "        train_concept_accuracy = 100 * train_concept_correct / train_total\n",
    "\n",
    "        # Calculate combined total accuracy as the average of concept and classification accuracies\n",
    "        train_total_accuracy = (train_concept_accuracy + train_accuracy) / 2\n",
    "\n",
    "        # Print stats for the current epoch\n",
    "        print(f\"\\nTrain Epoch {epoch+1}/{epochs}:\")\n",
    "        print(\"-------------------\")\n",
    "        print(f\"Concept Loss:          {train_concept_loss / len(train_loader):.4f} \\t Concept Accuracy:         {train_concept_accuracy:.2f}%\")\n",
    "        print(f\"Classification Loss:   {train_classification_loss / len(train_loader):.4f} \\t Classification Accuracy:  {train_accuracy:.2f}%\")\n",
    "        print(\"---------------------------------------------------------\")\n",
    "        print(f\"Total Loss:            {train_total_loss / len(train_loader):.4f} \\t Total Accuracy:            {train_total_accuracy:.2f}%\\n\")\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_total_loss = 0.0\n",
    "        val_concept_loss = 0.0\n",
    "        val_classification_loss = 0.0\n",
    "        val_total = 0\n",
    "        val_correct = 0\n",
    "        val_concept_correct = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, labels, concepts in val_loader:\n",
    "                images, labels, concepts = images.to(device), labels.to(device), concepts.to(device)\n",
    "\n",
    "                predicted_concepts, predictions = model(images)\n",
    "\n",
    "                loss_concept = criterion_concept(predicted_concepts, torch.argmax(concepts, dim=1))\n",
    "                loss_classification = criterion_classification(predictions, labels)\n",
    "                loss = lambda_concept * loss_concept + lambda_classification * loss_classification\n",
    "\n",
    "                val_total_loss += loss.item()\n",
    "                val_concept_loss += loss_concept.item()\n",
    "                val_classification_loss += loss_classification.item()\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predictions.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "                # Concept accuracy calculation\n",
    "                val_concept_correct += calculate_concept_accuracy(predicted_concepts, concepts)\n",
    "\n",
    "        # Calculate accuracies for validation\n",
    "        val_accuracy = 100 * val_correct / val_total\n",
    "        val_concept_accuracy = 100 * val_concept_correct / val_total\n",
    "\n",
    "        # Calculate combined total accuracy for validation\n",
    "        val_total_accuracy = (val_concept_accuracy + val_accuracy) / 2\n",
    "\n",
    "        # Print stats for the validation set\n",
    "        print(f\"Validate Epoch {epoch+1}/{epochs}:\")\n",
    "        print(\"-------------------\")\n",
    "        print(f\"Concept Loss:          {val_concept_loss / len(val_loader):.4f} \\t Concept Accuracy:         {val_concept_accuracy:.2f}%\")\n",
    "        print(f\"Classification Loss:   {val_classification_loss / len(val_loader):.4f} \\t Classification Accuracy:  {val_accuracy:.2f}%\")\n",
    "        print(\"---------------------------------------------------------\")\n",
    "        print(f\"Total Loss:            {val_total_loss / len(val_loader):.4f} \\t Total Accuracy:            {val_total_accuracy:.2f}%\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_cbm(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in tqdm(test_loader, desc=\"[Evaluate]\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            _, predictions = model(images)\n",
    "\n",
    "            _, predicted_labels = torch.max(predictions, 1)\n",
    "            total_correct += (predicted_labels == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    accuracy = 100 * total_correct / total_samples\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader, val_loader, test_loader = get_data_loaders(PREPROCESSED_DIR, CSV_CONCEPTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_concepts = len(pd.get_dummies(pd.read_csv(CSV_CONCEPTS_PATH)[[\"cap_color\", \"cap_shape\", \"cap_texture\", \"ring_present\", \"stem_thickness\"]]).columns)\n",
    "num_classes = len(pd.read_csv(CSV_CONCEPTS_PATH)['Mushroom'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConceptBottleneckModel(num_concepts, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.AdamW(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Joint Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Joint Training: Epoch 1/20]:   0%|          | 0/52 [00:00<?, ?it/s]C:\\Users\\ilian\\AppData\\Local\\Temp\\ipykernel_21908\\2125396519.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  image = torch.load(image_path)\n",
      "[Joint Training: Epoch 1/20]: 100%|██████████| 52/52 [00:33<00:00,  1.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch 1/20:\n",
      "-------------------\n",
      "Concept Loss:          1.5831 \t Concept Accuracy:         30.27%\n",
      "Classification Loss:   2.2814 \t Classification Accuracy:  14.02%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            2.0719 \t Total Accuracy:            14.02%\n",
      "\n",
      "Validate Epoch 1/20:\n",
      "-------------------\n",
      "Concept Loss:          1.2807 \t Concept Accuracy:         39.72%\n",
      "Classification Loss:   2.2182 \t Classification Accuracy:  19.15%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            1.9369 \t Total Accuracy:            19.15%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Joint Training: Epoch 2/20]: 100%|██████████| 52/52 [00:35<00:00,  1.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch 2/20:\n",
      "-------------------\n",
      "Concept Loss:          1.1972 \t Concept Accuracy:         48.40%\n",
      "Classification Loss:   2.0995 \t Classification Accuracy:  33.53%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            1.8288 \t Total Accuracy:            33.53%\n",
      "\n",
      "Validate Epoch 2/20:\n",
      "-------------------\n",
      "Concept Loss:          1.0728 \t Concept Accuracy:         57.75%\n",
      "Classification Loss:   1.9322 \t Classification Accuracy:  49.58%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            1.6744 \t Total Accuracy:            49.58%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Joint Training: Epoch 3/20]: 100%|██████████| 52/52 [00:42<00:00,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch 3/20:\n",
      "-------------------\n",
      "Concept Loss:          1.0627 \t Concept Accuracy:         57.52%\n",
      "Classification Loss:   1.7785 \t Classification Accuracy:  48.04%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            1.5638 \t Total Accuracy:            48.04%\n",
      "\n",
      "Validate Epoch 3/20:\n",
      "-------------------\n",
      "Concept Loss:          0.9520 \t Concept Accuracy:         63.38%\n",
      "Classification Loss:   1.5948 \t Classification Accuracy:  52.39%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            1.4020 \t Total Accuracy:            52.39%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Joint Training: Epoch 4/20]: 100%|██████████| 52/52 [00:40<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch 4/20:\n",
      "-------------------\n",
      "Concept Loss:          0.9646 \t Concept Accuracy:         63.93%\n",
      "Classification Loss:   1.4443 \t Classification Accuracy:  56.86%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            1.3004 \t Total Accuracy:            56.86%\n",
      "\n",
      "Validate Epoch 4/20:\n",
      "-------------------\n",
      "Concept Loss:          0.8902 \t Concept Accuracy:         67.32%\n",
      "Classification Loss:   1.3255 \t Classification Accuracy:  59.44%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            1.1949 \t Total Accuracy:            59.44%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Joint Training: Epoch 5/20]: 100%|██████████| 52/52 [00:43<00:00,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch 5/20:\n",
      "-------------------\n",
      "Concept Loss:          0.9097 \t Concept Accuracy:         65.38%\n",
      "Classification Loss:   1.2111 \t Classification Accuracy:  64.83%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            1.1207 \t Total Accuracy:            64.83%\n",
      "\n",
      "Validate Epoch 5/20:\n",
      "-------------------\n",
      "Concept Loss:          0.8445 \t Concept Accuracy:         67.32%\n",
      "Classification Loss:   1.2096 \t Classification Accuracy:  61.13%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            1.1000 \t Total Accuracy:            61.13%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Joint Training: Epoch 6/20]: 100%|██████████| 52/52 [00:44<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch 6/20:\n",
      "-------------------\n",
      "Concept Loss:          0.8736 \t Concept Accuracy:         66.53%\n",
      "Classification Loss:   1.1038 \t Classification Accuracy:  66.47%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            1.0347 \t Total Accuracy:            66.47%\n",
      "\n",
      "Validate Epoch 6/20:\n",
      "-------------------\n",
      "Concept Loss:          0.8350 \t Concept Accuracy:         67.89%\n",
      "Classification Loss:   1.1089 \t Classification Accuracy:  64.23%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            1.0267 \t Total Accuracy:            64.23%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Joint Training: Epoch 7/20]: 100%|██████████| 52/52 [00:44<00:00,  1.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch 7/20:\n",
      "-------------------\n",
      "Concept Loss:          0.8387 \t Concept Accuracy:         67.98%\n",
      "Classification Loss:   0.9937 \t Classification Accuracy:  68.88%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            0.9472 \t Total Accuracy:            68.88%\n",
      "\n",
      "Validate Epoch 7/20:\n",
      "-------------------\n",
      "Concept Loss:          0.8137 \t Concept Accuracy:         67.04%\n",
      "Classification Loss:   1.0423 \t Classification Accuracy:  64.51%\n",
      "---------------------------------------------------------\n",
      "Total Loss:            0.9737 \t Total Accuracy:            64.51%\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Joint Training: Epoch 8/20]:   6%|▌         | 3/52 [00:07<01:56,  2.39s/it]"
     ]
    }
   ],
   "source": [
    "print(\"Starting Joint Training\")\n",
    "train_jointly(model, train_loader, val_loader, device, optimizer, epochs=20, lambda_concept=0.3, lambda_classification=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating Model\")\n",
    "evaluate_cbm(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Intervention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image_and_predicted_concepts(model, image, true_label, concepts, concept_columns, device, transform=None):\n",
    "    if transform:\n",
    "        image = transform(image).unsqueeze(0)\n",
    "    image = image.to(device)\n",
    "    concepts = torch.tensor(concepts, dtype=torch.float32).to(device)\n",
    "\n",
    "    pil_image = ToPILImage()(image.cpu().squeeze(0))\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(pil_image)\n",
    "    plt.title(f\"Original Image (True label: {CLASS_NAMES[true_label]})\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        _, predictions = model(image)\n",
    "        predicted_class = predictions.argmax(dim=1).cpu().item()\n",
    "        predicted_concepts = concepts.cpu().numpy()\n",
    "\n",
    "    print(f\"Predicted Concepts (before intervention):\")\n",
    "    for idx, concept in enumerate(concept_columns):\n",
    "        print(f\"{concept}: {predicted_concepts[0][idx]}\")\n",
    "\n",
    "    print(f\"Original prediction: {CLASS_NAMES[predicted_class]}\")\n",
    "    return predicted_concepts, predicted_class\n",
    "\n",
    "\n",
    "def intervene_and_show_change(model, image, true_label, concepts, concept_columns, concept_column, device, transform=None):\n",
    "    concept_idx = concept_columns.index(concept_column)\n",
    "    predicted_concepts, predicted_class = show_image_and_predicted_concepts(model, image, true_label, concepts, concept_columns, device, transform)\n",
    "    modified_concepts = concepts.clone()\n",
    "    modified_concepts[concept_idx] = 1 - modified_concepts[concept_idx]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        modified_concepts = modified_concepts.unsqueeze(0).to(device)\n",
    "        _, predictions = model(image)\n",
    "        modified_pred_class = predictions.argmax(dim=1).cpu().item()\n",
    "\n",
    "    print(f\"\\nAfter intervening on '{concept_column}':\")\n",
    "    for idx, concept in enumerate(concept_columns):\n",
    "        print(f\"{concept}: {modified_concepts[0][idx].item()}\")\n",
    "\n",
    "    print(f\"Modified prediction: {CLASS_NAMES[modified_pred_class]}\")\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar([CLASS_NAMES[predicted_class], CLASS_NAMES[modified_pred_class]], [1, 1], color=['blue', 'orange'])\n",
    "    plt.title(f\"Prediction Change after Intervening on {concept_column}\")\n",
    "    plt.ylabel(\"Prediction\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image, true_label, concepts = next(iter(test_loader))\n",
    "concept_columns = [\"cap_color\", \"cap_shape\", \"cap_texture\", \"ring_present\", \"stem_thickness\"]\n",
    "random_concept_column = random.choice(concept_columns)\n",
    "\n",
    "intervene_and_show_change(\n",
    "    model=model,\n",
    "    image=image[0],\n",
    "    true_label=true_label[0].item(),\n",
    "    concepts=concepts[0],\n",
    "    concept_columns=concept_columns,\n",
    "    concept_column=random_concept_column,\n",
    "    device=device,\n",
    "    transform=None\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
